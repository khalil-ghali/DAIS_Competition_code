{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of necessary libraries and packages"
      ],
      "metadata": {
        "id": "gOPv0XciGG7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAcjuOfpxhIl",
        "outputId": "d851af23-4cc2-4b5a-fbb5-efc573649ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.281\n",
            "  Downloading langchain-0.0.281-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.3.29\n",
            "  Downloading chromadb-0.3.29-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting InstructorEmbedding\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.31.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyPDF\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.281)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.281)\n",
            "  Downloading langsmith-0.0.89-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.9.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (8.2.3)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (1.5.3)\n",
            "Collecting pydantic<3,>=1 (from langchain==0.0.281)\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.7 (from chromadb==0.3.29)\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.29)\n",
            "  Downloading clickhouse_connect-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (0.9.2)\n",
            "Collecting fastapi==0.85.1 (from chromadb==0.3.29)\n",
            "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.29)\n",
            "  Downloading uvicorn-0.27.0.post1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.3.29)\n",
            "  Downloading posthog-3.4.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.29)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.3.29)\n",
            "  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (0.15.1)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.29) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.3.29)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Collecting starlette==0.20.4 (from fastapi==0.85.1->chromadb==0.3.29)\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.12.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.281) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.281) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.281) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.281) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.281) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2024.2.2)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2023.4)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.29)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.29)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.29)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.29)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.29)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.281) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.281) (3.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.281) (3.0.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.29)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.17.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.2.0)\n",
            "Building wheels for collected packages: langdetect, hnswlib\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=ad837b2b2216bb301b058e2445afd626d5e19a1dbbd48edb5d47b521249286c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287617 sha256=ecbdaa2f8dd859ecfab5e14925bb14101d3d055d1e2c68808665d454c1cc0c1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built langdetect hnswlib\n",
            "Installing collected packages: monotonic, InstructorEmbedding, zstandard, websockets, watchdog, validators, uvloop, smmap, python-dotenv, pyPDF, pydantic, pulsar-client, overrides, mypy-extensions, marshmallow, lz4, langdetect, humanfriendly, httptools, hnswlib, h11, backoff, youtube-transcript-api, watchfiles, uvicorn, typing-inspect, starlette, pydeck, posthog, langsmith, gitdb, deep_translator, coloredlogs, clickhouse-connect, onnxruntime, gitpython, fastapi, dataclasses-json, langchain, chromadb, streamlit, sentence-transformers\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 backoff-2.2.1 chromadb-0.3.29 clickhouse-connect-0.7.0 coloredlogs-15.0.1 dataclasses-json-0.5.14 deep_translator-1.11.4 fastapi-0.85.1 gitdb-4.0.11 gitpython-3.1.41 h11-0.14.0 hnswlib-0.8.0 httptools-0.6.1 humanfriendly-10.0 langchain-0.0.281 langdetect-1.0.9 langsmith-0.0.89 lz4-4.3.3 marshmallow-3.20.2 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.0 overrides-7.7.0 posthog-3.4.0 pulsar-client-3.4.0 pyPDF-4.0.1 pydantic-1.10.14 pydeck-0.8.1b0 python-dotenv-1.0.1 sentence-transformers-2.3.1 smmap-5.0.1 starlette-0.20.4 streamlit-1.31.0 typing-inspect-0.9.0 uvicorn-0.27.0.post1 uvloop-0.19.0 validators-0.22.0 watchdog-4.0.0 watchfiles-0.21.0 websockets-12.0 youtube-transcript-api-0.6.2 zstandard-0.22.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain==0.0.281 chromadb==0.3.29 transformers sentence-transformers  InstructorEmbedding streamlit pyPDF youtube-transcript-api deep_translator langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load the app to a file app.py to run it as containarized a web app"
      ],
      "metadata": {
        "id": "HvHnFB2jGWGj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIDws74fyEaI",
        "outputId": "8464d91b-0df9-4171-b7b6-de53aefad175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "def chat_with_documents():\n",
        "\n",
        "        import streamlit as st\n",
        "\n",
        "        from langchain.vectorstores import FAISS\n",
        "        import base64\n",
        "        #from PyPDF2 import PdfReader\n",
        "        from langchain.document_loaders import PyPDFLoader\n",
        "        import torch\n",
        "        import os\n",
        "        #from apikey import apikey\n",
        "        from langchain.document_loaders import TextLoader\n",
        "        from langchain.indexes import VectorstoreIndexCreator\n",
        "        from langchain.text_splitter import CharacterTextSplitter\n",
        "        import time\n",
        "        from langchain import HuggingFaceHub\n",
        "        from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "        from langchain.vectorstores import Chroma\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "        from langchain.chains import RetrievalQA\n",
        "        import textwrap\n",
        "        import os\n",
        "        from langchain.document_loaders import DirectoryLoader\n",
        "        import shutil\n",
        "        from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "        from deep_translator import GoogleTranslator\n",
        "        from langdetect import detect\n",
        "        HUGGINGFACE_API_TOKEN = \"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\"\n",
        "        repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "        st.title(\"TextGenius: Your Research Chat Buddy üìÑü§ñ\")\n",
        "        llm=HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN,\n",
        "                                repo_id=repo_id,\n",
        "                                model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700})\n",
        "\n",
        "\n",
        "        pdfs_directory = \"PDFs\"\n",
        "        if not os.path.exists(pdfs_directory):\n",
        "            os.makedirs(pdfs_directory)\n",
        "        for file_name in os.listdir(pdfs_directory):\n",
        "                                file_path = os.path.join(pdfs_directory, file_name)\n",
        "                                if os.path.isfile(file_path):\n",
        "                                    os.remove(file_path)\n",
        "        #Free_Open Source Model\n",
        "\n",
        "        if 'exit' not in st.session_state:\n",
        "                st.session_state['exit'] = False\n",
        "        def typewriter(text: str, speed: float):\n",
        "            container = st.empty()\n",
        "            displayed_text = \"\"\n",
        "\n",
        "            for char in text:\n",
        "                displayed_text += char\n",
        "                container.markdown(displayed_text)\n",
        "                time.sleep(1/speed)\n",
        "        def wrap_text_preserve_newlines(text, width=110):\n",
        "            # Split the input text into lines based on newline characters\n",
        "            lines = text.split('\\n')\n",
        "\n",
        "            # Wrap each line individually\n",
        "            wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "            # Join the wrapped lines back together using newline characters\n",
        "            wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "            return wrapped_text\n",
        "                # do something with the data\n",
        "        def process_llm_response(llm_response,llm_originalresponse2):\n",
        "            result_text = wrap_text_preserve_newlines(llm_originalresponse2)\n",
        "            typewriter(result_text, speed=40)\n",
        "        def process_source (llm_response):\n",
        "            st.write('\\n\\nSources:')\n",
        "            unique_sources = []\n",
        "            for source in llm_response[\"source_documents\"]:\n",
        "                source_name = source.metadata['source']\n",
        "                if source_name not in unique_sources:\n",
        "                    unique_sources.append(source_name)\n",
        "            for source in unique_sources:\n",
        "                        pdf_display = display_pdf(source)\n",
        "                        st.markdown(pdf_display, unsafe_allow_html=True)\n",
        "        def display_pdf(file_path):\n",
        "            \"\"\"Display PDF file.\n",
        "\n",
        "            Args:\n",
        "                file_path (str): Path to the PDF file.\n",
        "\n",
        "            Returns:\n",
        "                str: PDF display in HTML format.\n",
        "            \"\"\"\n",
        "\n",
        "            with open(file_path, \"rb\") as f:\n",
        "                base64_pdf = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "            pdf_display = (\n",
        "                f'<embed src=\"data:application/pdf;base64,{base64_pdf}\" width=\"100%\" height=\"600\" type=\"application/pdf\">'\n",
        "            )\n",
        "            return pdf_display\n",
        "\n",
        "        def save_uploaded_pdfs(uploaded_files):\n",
        "        # Save uploaded PDF files to the \"PDFs\" directory\n",
        "            if uploaded_files:\n",
        "                    for uploaded_file in uploaded_files:\n",
        "                        original_filename = uploaded_file.name  # Get the original filename\n",
        "                        unique_filename = original_filename\n",
        "                        pdf_path = os.path.join(pdfs_directory, unique_filename)\n",
        "\n",
        "                        # Extract the content from the UploadedFile\n",
        "                        file_content = uploaded_file.read()\n",
        "\n",
        "                        with open(pdf_path, \"wb\") as pdf_file:\n",
        "                            pdf_file.write(file_content)\n",
        "                        success_message = st.empty()\n",
        "                        success_message.success(f\"File '{unique_filename}' successfully uploaded.\")\n",
        "                        time.sleep(10)  # Adjust the duration as needed\n",
        "                        success_message.empty()\n",
        "        def launchdoc():\n",
        "\n",
        "            uploaded_files = st.file_uploader(\"Please upload all your documents at once\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "            original_question = st.text_input(\"Once uploaded, you can chat with your document. Enter your question here or type exit to end and upload new documents:\")\n",
        "            question = GoogleTranslator(source='auto', target='en').translate(original_question)\n",
        "            submit_button = st.button('Generate Response ü™Ñ‚ú®')\n",
        "            if uploaded_files and submit_button:\n",
        "                                save_uploaded_pdfs(uploaded_files)\n",
        "                                loader = DirectoryLoader('./PDFs', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "                                with st.spinner('Processing the Documents...'):\n",
        "                                  documents = loader.load()\n",
        "                                  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "                                  texts = text_splitter.split_documents(documents)\n",
        "                                #instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                with st.spinner('Processing Embeddings...'):                                                          #model_kwargs={\"device\": \"cuda\"})\n",
        "                                  model_name = \"BAAI/bge-base-en\"\n",
        "                                  encode_kwargs = {'normalize_embeddings': True}\n",
        "                                  instructor_embeddings =instructor_embeddings = HuggingFaceBgeEmbeddings(\n",
        "      model_name=model_name,\n",
        "      model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
        "      encode_kwargs=encode_kwargs\n",
        "  )\n",
        "                                  persist_directory = 'db'\n",
        "\n",
        "                                  ## Here is the new embeddings being used\n",
        "                                  embedding = instructor_embeddings\n",
        "                                  if os.path.exists(persist_directory):\n",
        "                                      shutil.rmtree(persist_directory)\n",
        "                                  vectordb = Chroma.from_documents(documents=texts,embedding=embedding, persist_directory=persist_directory)\n",
        "                                  #vectordb= FAISS.from_documents(texts, embedding)\n",
        "                                  retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "                                  qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                                                      chain_type=\"stuff\",\n",
        "                                                                      retriever=retriever,\n",
        "                                                                      return_source_documents=True)\n",
        "                                # Initial state\n",
        "\n",
        "                                while st.session_state['exit'] == False:\n",
        "\n",
        "                                            #question = st.text_input(\"Once uploaded, you can chat with your document. Enter your question here or type exit to end and upload new documents:\", key=f\"question_input_{i}\")\n",
        "                                            with st.spinner('Generating Answer...'):\n",
        "\n",
        "                                                if question.lower() == 'exit':\n",
        "                                                    st.session_state['exit'] = True\n",
        "\n",
        "                                                else:\n",
        "                                                    detected_source_language = detect(original_question)\n",
        "                                                    chunk_size = 5000\n",
        "                                                    # Process the question and display the response\n",
        "                                                    llm_originalresponse = qa_chain(question)\n",
        "                                                    llm_originalresponse2=str(llm_originalresponse['result'])\n",
        "\n",
        "                                                    chunks = [llm_originalresponse2[i:i+chunk_size] for i in range(0, len(llm_originalresponse2), chunk_size)]\n",
        "                                                    translated_chunks = []\n",
        "                                                    my_translator=GoogleTranslator(source='auto', target=detected_source_language)\n",
        "                                                    for chunk in chunks:\n",
        "                                                            translated_chunk = my_translator.translate(chunk)\n",
        "                                                            translated_chunks.append(translated_chunk)\n",
        "                                                    llm_originalresponse2=''.join(translated_chunks)\n",
        "                                                    process_llm_response(llm_originalresponse,llm_originalresponse2)\n",
        "                                                    process_source (llm_originalresponse)\n",
        "\n",
        "                                                    break\n",
        "\n",
        "                                                if st.session_state['exit'] == True:\n",
        "                                                    output=\"Thank you for trying our Tool, We hope you liked it\"\n",
        "                                                    typewriter(output, speed=5)\n",
        "                                                    # Delete files and folders\n",
        "                                                    for file_name in os.listdir(pdfs_directory):\n",
        "                                                        file_path = os.path.join(pdfs_directory, file_name)\n",
        "                                                        if os.path.isfile(file_path):\n",
        "                                                            os.remove(file_path)\n",
        "\n",
        "                                                    # Remove \"db\" directory\n",
        "\n",
        "\n",
        "                                                    break\n",
        "\n",
        "\n",
        "\n",
        "            st.warning(\"‚ö†Ô∏è Please Keep in mind that the accuracy of the response relies on the :red[PDF's Quality] and the :red[prompt's Quality]. Occasionally, the response may not be entirely accurate. Consider using the response as a reference rather than a definitive answer.\")\n",
        "\n",
        "\n",
        "        launchdoc()\n",
        "\n",
        "def chat_with_website():\n",
        "        import torch\n",
        "        import os\n",
        "        import argparse\n",
        "        import shutil\n",
        "        from langchain.document_loaders import YoutubeLoader\n",
        "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "        from langchain.vectorstores import Chroma\n",
        "        from langchain.embeddings import OpenAIEmbeddings\n",
        "        from langchain.chains import RetrievalQA\n",
        "        from langchain.llms import OpenAI\n",
        "        import streamlit as st\n",
        "        from langchain.chat_models import ChatOpenAI\n",
        "        from langchain import HuggingFaceHub\n",
        "        from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "        from urllib.parse import urlparse, parse_qs\n",
        "        from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "        from transformers import pipeline\n",
        "        import textwrap\n",
        "        import time\n",
        "        from deep_translator import GoogleTranslator\n",
        "        from langdetect import detect\n",
        "        from langchain.prompts.chat import (ChatPromptTemplate,\n",
        "                                            HumanMessagePromptTemplate,\n",
        "                                            SystemMessagePromptTemplate)\n",
        "        from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "\n",
        "        def typewriter(text: str, speed: float):\n",
        "                    container = st.empty()\n",
        "                    displayed_text = \"\"\n",
        "\n",
        "                    for char in text:\n",
        "                        displayed_text += char\n",
        "                        container.markdown(displayed_text)\n",
        "                        time.sleep(1/speed)\n",
        "        def wrap_text_preserve_newlines(text, width=110):\n",
        "                    # Split the input text into lines based on newline characters\n",
        "                    lines = text.split('\\n')\n",
        "\n",
        "                    # Wrap each line individually\n",
        "                    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "                    # Join the wrapped lines back together using newline characters\n",
        "                    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "                    return wrapped_text\n",
        "        def process_llm_response(llm_originalresponse2):\n",
        "                    #result_text = wrap_text_preserve_newlines(llm_originalresponse2[\"result\"])\n",
        "                    typewriter(llm_originalresponse2[\"result\"], speed=40)\n",
        "\n",
        "        def extract_video_id(youtube_url):\n",
        "            try:\n",
        "                parsed_url = urlparse(youtube_url)\n",
        "                query_params = parse_qs(parsed_url.query)\n",
        "                video_id = query_params.get('v', [None])[0]\n",
        "\n",
        "                return video_id\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting video ID: {e}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "\n",
        "        def launchwebsitecomponent():\n",
        "                HUGGINGFACE_API_TOKEN = \"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\"\n",
        "                model_name = \"BAAI/bge-base-en\"\n",
        "                encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "                st.title('TextGenius: Your Chat with Websites Assistant üåêü§ñ')\n",
        "\n",
        "                url = st.text_input(\"Insert the Website URL\", placeholder=\"Format should be like: https://platform.openai.com/account/api-keys.\")\n",
        "                query = st.text_input(\"Ask any question about the Website\",help=\"Suggested queries: Summarize the key points of this webpage - What is this website about - Ask about a specific thing in the webite \")\n",
        "\n",
        "\n",
        "                if st.button('Generate Response ü™Ñ‚ú®'):\n",
        "                  with st.spinner('Processing the Website Data...'):\n",
        "\n",
        "                      loader = WebBaseLoader(url)\n",
        "                      documents = loader.load()\n",
        "\n",
        "                      text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "                      documents = text_splitter.split_documents(documents)\n",
        "                      if os.path.exists('./data'):\n",
        "                          shutil.rmtree('./data')\n",
        "                      vectordb = Chroma.from_documents(\n",
        "                      documents,\n",
        "                      #embedding = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                                                            # model_kwargs={\"device\": \"cuda\"})\n",
        "                      embedding= HuggingFaceBgeEmbeddings( model_name=model_name, model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}, encode_kwargs=encode_kwargs)\n",
        "                  )\n",
        "\n",
        "                      repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "                      qa_chain = RetrievalQA.from_chain_type(\n",
        "\n",
        "                      llm=HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN,\n",
        "                                      repo_id=repo_id,\n",
        "                                      model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700}),\n",
        "                          retriever=vectordb.as_retriever(),\n",
        "                          return_source_documents=False,\n",
        "                          verbose=False\n",
        "                      )\n",
        "                  with st.spinner('Generating Answer...'):\n",
        "                        llm_response = qa_chain(query)\n",
        "                        #llm_originalresponse2=llm_response['result']\n",
        "                        process_llm_response(llm_response)\n",
        "                st.warning(\"‚ö†Ô∏è Please Keep in mind that the accuracy of the response relies on the :red[Website Layout] and the :red[prompt's Quality]. Occasionally, the response may not be entirely accurate. Consider using the response as a reference rather than a definitive answer.\")\n",
        "        launchwebsitecomponent()\n",
        "def chat_with_youtube():\n",
        "          import torch\n",
        "          import os\n",
        "          import argparse\n",
        "          import shutil\n",
        "          from langchain.document_loaders import YoutubeLoader\n",
        "          from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "          from langchain.vectorstores import Chroma\n",
        "          from langchain.embeddings import OpenAIEmbeddings\n",
        "          from langchain.chains import RetrievalQA\n",
        "          from langchain.llms import OpenAI\n",
        "          import streamlit as st\n",
        "          from langchain.chat_models import ChatOpenAI\n",
        "          from langchain import HuggingFaceHub\n",
        "          from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "          from urllib.parse import urlparse, parse_qs\n",
        "          from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "          from transformers import pipeline\n",
        "          import textwrap\n",
        "          import time\n",
        "          from deep_translator import GoogleTranslator\n",
        "          from langdetect import detect\n",
        "\n",
        "\n",
        "          def typewriter(text: str, speed: float):\n",
        "                      container = st.empty()\n",
        "                      displayed_text = \"\"\n",
        "\n",
        "                      for char in text:\n",
        "                          displayed_text += char\n",
        "                          container.markdown(displayed_text)\n",
        "                          time.sleep(1/speed)\n",
        "          def wrap_text_preserve_newlines(text, width=110):\n",
        "                      # Split the input text into lines based on newline characters\n",
        "                      lines = text.split('\\n')\n",
        "\n",
        "                      # Wrap each line individually\n",
        "                      wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "                      # Join the wrapped lines back together using newline characters\n",
        "                      wrapped_text = '\\n'.join(wrapped_lines)\n",
        "                      return wrapped_text\n",
        "          def process_llm_response(llm_originalresponse2):\n",
        "                      #result_text = wrap_text_preserve_newlines(llm_originalresponse2[\"result\"])\n",
        "                      typewriter(llm_originalresponse2[\"result\"], speed=40)\n",
        "\n",
        "          def extract_video_id(youtube_url):\n",
        "              try:\n",
        "                  parsed_url = urlparse(youtube_url)\n",
        "                  query_params = parse_qs(parsed_url.query)\n",
        "                  video_id = query_params.get('v', [None])[0]\n",
        "\n",
        "                  return video_id\n",
        "              except Exception as e:\n",
        "                  print(f\"Error extracting video ID: {e}\")\n",
        "                  return None\n",
        "\n",
        "\n",
        "\n",
        "          def launchyoutubecomponent():\n",
        "                  HUGGINGFACE_API_TOKEN = \"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\"\n",
        "                  model_name = \"BAAI/bge-base-en\"\n",
        "                  encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "                  st.title('TextGenius: Your Chat with Youtube Assistant')\n",
        "\n",
        "                  videourl = st.text_input(\"Insert The video URL\",  placeholder=\"Format should be like: https://www.youtube.com/watch?v=pSLeYvld8Mk\")\n",
        "                  query = st.text_input(\"Ask any question about the video\",help=\"Suggested queries: Summarize the key points of this video - What is this video about - Ask about a specific thing in the video \")\n",
        "\n",
        "\n",
        "                  if st.button('Generate Response ü™Ñ‚ú®'):\n",
        "                    with st.spinner('Processing the Video...'):\n",
        "                        video_id = extract_video_id(videourl)\n",
        "                        loader = YoutubeLoader(video_id)\n",
        "                        documents = loader.load()\n",
        "\n",
        "                        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "                        documents = text_splitter.split_documents(documents)\n",
        "                        persist_directory='db1'\n",
        "                        if os.path.exists(persist_directory):\n",
        "                            shutil.rmtree(persist_directory)\n",
        "                        vectordb = Chroma.from_documents(\n",
        "                        documents,\n",
        "                        #embedding = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                                                              # model_kwargs={\"device\": \"cuda\"})\n",
        "                        embedding= HuggingFaceBgeEmbeddings( model_name=model_name, model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}, encode_kwargs=encode_kwargs),persist_directory=persist_directory\n",
        "                    )\n",
        "\n",
        "                        repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "                        qa_chain = RetrievalQA.from_chain_type(\n",
        "\n",
        "                        llm=HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN,\n",
        "                                        repo_id=repo_id,\n",
        "                                        model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700}),\n",
        "                            retriever=vectordb.as_retriever(),\n",
        "                            return_source_documents=False,\n",
        "                            verbose=False\n",
        "                        )\n",
        "                    with st.spinner('Generating Answer...'):\n",
        "                          llm_response = qa_chain(query)\n",
        "                          #llm_originalresponse2=llm_response['result']\n",
        "                          process_llm_response(llm_response)\n",
        "                  st.warning(\"‚ö†Ô∏è Please Keep in mind that the accuracy of the response relies on the :red[Video's quality] and the :red[prompt's Quality]. Occasionally, the response may not be entirely accurate. Consider using the response as a reference rather than a definitive answer.\")\n",
        "          launchyoutubecomponent()\n",
        "def intro():\n",
        "            st.markdown(\"\"\"\n",
        "            # Welcome to TextGenius\n",
        "\n",
        "            TextGenius is an innovative web application designed to harness the power of advanced Large Language Models (LLMs) and provide users with an intuitive platform for interacting with text data. By leveraging cutting-edge AI technology, TextGenius simplifies the extraction of insights from documents, YouTube videos, and website content. Whether you are an academic researcher, industry professional, or student, TextGenius offers a tailored experience that elevates your data interaction to new heights.\n",
        "\n",
        "            ## Base Models\n",
        "\n",
        "            Q&A-Assistant is built on Falcon 7B instruct Model to enhance your research experience. Whether you're a student, researcher, or professional, we're here to simplify your interactions with your documents. üí°üìö\n",
        "\n",
        "            ## Standout Features\n",
        "\n",
        "            - AI-Powered Q&A: Upload your PDF , Input a Youtube video or website Link,  Get precise answers like a personal Q&A expert! üí≠ü§ñ\n",
        "\n",
        "            ## How to Get Started\n",
        "\n",
        "            1. Upload your Document, or Input a Youtube video or website Link\n",
        "            3. Ask questions using everyday language using your favorite language\n",
        "            4. Get detailed, AI-generated answers.\n",
        "            5. Enjoy a smarter way to interact with text data!\n",
        "\n",
        "\n",
        "            ## It is Time to Dive in! Welcome aboard the journey to a smarter way of handling text data.\n",
        "\n",
        "\n",
        "            \"\"\")\n",
        "page_names_to_funcs = {\n",
        "    \"Main Page\": intro,\n",
        "    \"Chat with Documents\": chat_with_documents,\n",
        "    \"Chat with Youtube Videos\": chat_with_youtube,\n",
        "    \"Chat with a Website\": chat_with_website\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "demo_name = st.sidebar.selectbox(\"Please choose your tool üòä \", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[demo_name]()\n",
        "st.sidebar.markdown('<a href=\"https://www.linkedin.com/in/mohammed-khalil-ghali-11305119b/\"> Connect on LinkedIn <img src=\"https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linkedin/linkedin-original.svg\" alt=\"LinkedIn\" width=\"30\" height=\"30\"></a>', unsafe_allow_html=True)\n",
        "st.sidebar.markdown('<a href=\"https://github.com/khalil-ghali\"> Check out my GitHub <img src=\"https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg\" alt=\"GitHub\" width=\"30\" height=\"30\"></a>', unsafe_allow_html=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Local Tunnel will alow us to create a dummy tunnel to run our streamlit app"
      ],
      "metadata": {
        "id": "w30SJBp5Ghht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZwlrHBn9_MO",
        "outputId": "b42bf7df-c840-4228-e617-ac0ce5affebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.576s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the app"
      ],
      "metadata": {
        "id": "oMzd76WkGsZc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfAZMlEH-Bt5"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Password to input in the tunnel website is gotten by running the cell below, it should be X.X.X.X"
      ],
      "metadata": {
        "id": "pHBtb1NEGv-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYMrDm_G-ECA",
        "outputId": "7d25371e-1946-4e65-c377-8031d3843b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.145.45.166"
          ]
        }
      ],
      "source": [
        "!curl ipecho.net/plain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run the cell below to get the temporary url of the app. once you access the link you will be prompted for a password that is the output of the previous cell"
      ],
      "metadata": {
        "id": "WyZE-7wGG75n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILbOGp8--GMQ",
        "outputId": "b22d9e86-5761-4f9a-dff3-0d4eb7e21f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.25s\n",
            "your url is: https://every-beans-notice.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}